{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e78974-5f96-47e7-a6c7-a1271e49a9ef",
   "metadata": {},
   "source": [
    "# Classifying images\n",
    "In this tutorial you will learn how deep learning for image classification works and experiment with a very small convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d6ae78-bd7d-4a07-a4a8-5d261cb78bba",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a62e2-9080-4b94-8fc1-055d420ffac7",
   "metadata": {},
   "source": [
    "First, we'll import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79def733-c4d5-45d0-bd80-5ff2b5b2325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# Keras utilities\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e2ee9-9c59-49e1-ad6b-cd4fe7aa7b05",
   "metadata": {},
   "source": [
    "Now we'll import some utilities for visualisation etc. If you want to take a look at this code, uncomment the second line below before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2736a0a-c993-4684-b5ba-554f79660be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from files import utils\n",
    "# %load files/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0e491-4a99-4ad7-9712-8e020432e4df",
   "metadata": {},
   "source": [
    "## A simple model\n",
    "The code in this section creates a very simple convolutional neural network that we can play around with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bded242-7790-47eb-8234-1c3e76f44f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple example network for visualising the filters and layers.\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 2\n",
    "image_size = 8\n",
    "\n",
    "num_filters = 8 # also 4, 16\n",
    "num_filters_l2 = 16\n",
    "filter_size = 3 # also 4,5\n",
    "# num_hidden = 64 # also 16?)\n",
    "\n",
    "# batch_size = 5 # Half the images sent in each mini-batch\n",
    "#batch_size = 10 # All images in each batch\n",
    "batch_size = 1 # One at a time\n",
    "# batch_size = 2 # 2 at a time\n",
    "\n",
    "# initializer = initializers.Constant(0.1)\n",
    "kernel_init = initializers.RandomUniform(minval=-0.01, maxval=0.01)\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\"\"\"\n",
    "data_augmentation = Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomFlip(\"vertical\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.5, interpolation='nearest', fill_mode='constant', fill_value=0.0),\n",
    "    ]\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        layers.Input((image_size, image_size, 3)),\n",
    "        # data_augmentation,\n",
    "        layers.Rescaling(1.0 / 255),                       # Rescale to 0..1\n",
    "        layers.Conv2D(num_filters, filter_size, padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init),\n",
    "        layers.MaxPooling2D(), # Halve the resolution to 4x4\n",
    "        layers.Conv2D(num_filters_l2, filter_size, padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init),\n",
    "        layers.MaxPooling2D(), # Halve the resolution to 2x2\n",
    "        # layers.Conv2D(num_filters, filter_size, padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init),\n",
    "        # layers.MaxPooling2D(),\n",
    "        # layers.Dropout(0.2),\n",
    "        layers.Flatten(),                                                                       # Flatten filters into a 1D vector of vars\n",
    "        ##layers.Dense(num_hidden, activation=\"relu\"),                                            # Hidden layer\n",
    "        layers.Dense(num_classes, activation=\"softmax\", kernel_initializer=kernel_init),                                        # output layer\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.save_weights('files/model.weights.h5') # Save the initialised weights so we can reset them later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516ac9d-d010-4848-930a-49bac5d9a3c6",
   "metadata": {},
   "source": [
    "## Visualising the model\n",
    "We can get an overview of the structure of thje model using model.summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e114e66-a933-4996-951d-98a33f5e87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(line_length=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c093aa-1a20-446c-bee6-0b268ab47ad9",
   "metadata": {},
   "source": [
    "The model (network) has now been created, but it hasn't been trained yet; the convolutional filter weights have been initialised to small random numbers. Let's see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336bfe9-b1be-437c-a623-1bb6de1ee980",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218379c4-d247-40b5-9381-8a8c4b18c4b1",
   "metadata": {},
   "source": [
    "## training the model\n",
    "Let's try training our model. We'll create two dataset generators to process the images into tensors and pass them into the model.\n",
    "The model.fit(...) call then trains the model. The 'epochs' parameter controls how much data is fed into the model during training. We'll start with a short training run.\n",
    "Note: this is a very small model, and the dataset is also small (both the image size and number of images). Normally we would expect training to take much longer, and would use a GPU to greatly speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a66972-ecdb-4e64-a9bf-37b3735f3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some training\n",
    "\n",
    "# data_dir = \"images_noisy_RGB_train\"\n",
    "# train_data_dir = \"images_clean/train\"  # Clean training data\n",
    "# train_data_dir = \"images_noisy_GB_train/train\"  # Random FP in training set\n",
    "data_dir = \"files/images_red\"  # Single-channel (red) images\n",
    "\n",
    "train_data_dir = data_dir + \"/train\"\n",
    "valid_data_dir = data_dir + \"/val\"\n",
    "test_data_dir = data_dir + \"/test\"\n",
    "\n",
    "train_generator = tf.keras.utils.image_dataset_from_directory(train_data_dir, image_size=(image_size,image_size), batch_size = batch_size)\n",
    "valid_generator = tf.keras.utils.image_dataset_from_directory(valid_data_dir, image_size=(image_size,image_size), batch_size = batch_size)\n",
    "    \n",
    "# model.fit_generator(train_generator, epochs=fine_tune_epochs, validation_data=valid_generator, callbacks = callbacks)\n",
    "model.fit(train_generator, validation_data=valid_generator, epochs=EPOCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17e4db-6fe6-400e-ab34-edf7ca37d4f4",
   "metadata": {},
   "source": [
    "Let's see how it does on some test images that were witheld from training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7402c01-cf39-49d5-bf93-696d6ddfb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_images = os.listdir(test_data_dir)\n",
    "class_names = ['X', '0']\n",
    "tot_correct = 0\n",
    "for image_file in test_images:\n",
    "    img = tf.keras.utils.load_img(os.path.join(test_data_dir, image_file), target_size=(image_size, image_size))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "    predictions = model.predict(img_array)\n",
    "    scores = tf.nn.softmax(predictions[0])\n",
    "    score = np.max(scores)\n",
    "    ans = class_names[np.argmax(scores)]\n",
    "    correct = (ans == image_file[0]) # Filename is prefixed with class\n",
    "    tot_correct += correct\n",
    "    print(f\"{image_file}: {ans} {score} {correct}\")\n",
    "print(\"==================================\")\n",
    "print(f\"{tot_correct} of {len(test_images)} correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13720e-693d-4ad3-a30a-65012d426437",
   "metadata": {},
   "source": [
    "What happened to the weights during training? Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8bb0b9-b61a-4582-b674-112637892f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce84823-c8eb-47fc-8e7a-22ba1bb455e4",
   "metadata": {},
   "source": [
    "The weights for the first layer of the network represent filters that extract low-level predictive features from the images, such as straight and curved lines. Our training data was monochrome (red), so for the green and blue channels the filters are (almost) empty. Unfortunately, visualising the next level of filters is less informative because they are (collectively) identifying patterns in the outputs from all of the filters in the previous layers. This information is highly distributed and difficult to decipher.\n",
    "\n",
    "We can, however, also look at the *output* from the filters:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b054d-8681-421f-beba-feec55122043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \"X\"\n",
    "utils.report_outputs(model, os.path.join(test_data_dir, test_images[8]), image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb846c4-4522-4f88-a448-ffcb0826325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \"O\"\n",
    "utils.report_outputs(model, os.path.join(test_data_dir, test_images[3]), image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0129c763-3936-461c-8788-923e57e12452",
   "metadata": {},
   "source": [
    "What are the filters 'looking at'? Each filter highlights the areas where the filter pattern matches, and dims the areas of the image where a match is poor. For example, the 'cross' image may have a particular diagonal highlighted, because this feature is found only in this class. For the 'nought' image, this same diagonal pattern picks up opposite 'corners' of the 'O' shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e83472-1b6c-4e50-94a0-61b2ade57aa6",
   "metadata": {},
   "source": [
    "## Explaining the model - attribute importance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c4356-3b15-4483-8cec-631fa63680dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.test_marginal_perm(test_data_dir, model, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848bdf7-a262-46d2-a6b1-aeadc9a0020a",
   "metadata": {},
   "source": [
    "## Noisy data\n",
    "In the example above, the green and blue bands were uninformative because during training they were empty. Let's see what happens if we instead pass in noisy data. Let's see what it looks like compared to the \"pure\" red image used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8604a6a3-90fd-47ad-b647-25a7dce96c91",
   "metadata": {},
   "source": [
    "<img src=\"files/images_red/train/nought/0_1.png\" width=\"100\"/>\n",
    "<img src=\"files/images_noisy/train/nought/0_1.png\" width=\"100\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35bc1a5-15fc-4a08-b72f-68b40706085a",
   "metadata": {},
   "source": [
    "The noisy images have random noise added to the green and blue channels - both false negatives and positives - but the red channel is still clean.\n",
    "\n",
    "Let's see what happens when we retrain the model on this data.\n",
    "\n",
    "First we'll recompile the model to reset the weights to random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d9811-a99d-4085-a73f-c62ac93273eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('files/model.weights.h5', skip_mismatch=True)\n",
    "utils.report_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437e223-c0f0-4be8-85b2-b8de4bdc6cc7",
   "metadata": {},
   "source": [
    "Now we'll retrain the model using the randomised imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82be17-974b-481c-a861-37001e984238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "data_dir = \"files/images_noisy\"\n",
    "\n",
    "train_data_dir = data_dir + \"/train\"\n",
    "valid_data_dir = data_dir + \"/val\"\n",
    "test_data_dir = data_dir + \"/test\"\n",
    "\n",
    "train_generator = tf.keras.utils.image_dataset_from_directory(train_data_dir, image_size=(image_size,image_size), batch_size = batch_size)\n",
    "valid_generator = tf.keras.utils.image_dataset_from_directory(valid_data_dir, image_size=(image_size,image_size), batch_size = batch_size)\n",
    "    \n",
    "model.fit(train_generator, validation_data=valid_generator, epochs=EPOCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a851b-20ba-46b1-9058-5b6b07d66486",
   "metadata": {},
   "source": [
    "How did it do? Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3c460-73ea-4bef-b7c3-9ccbeacd131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "print(test_data_dir)\n",
    "\n",
    "test_images = os.listdir(test_data_dir)\n",
    "class_names = ['X', '0']\n",
    "tot_correct = 0\n",
    "for image_file in test_images:\n",
    "    img = tf.keras.utils.load_img(os.path.join(test_data_dir, image_file), target_size=(image_size, image_size))\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "    predictions = model.predict(img_array)\n",
    "    scores = tf.nn.softmax(predictions[0])\n",
    "    score = np.max(scores)\n",
    "    ans = class_names[np.argmax(scores)]\n",
    "    correct = (ans == image_file[0]) # Filename is prefixed with class\n",
    "    tot_correct += correct\n",
    "    print(f\"{image_file}: {ans} {score} {correct}\")\n",
    "print(\"==================================\")\n",
    "print(f\"{tot_correct} of {len(test_images)} correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66bcb4-bda1-4570-a486-c62c7b894952",
   "metadata": {},
   "source": [
    "What do the weights look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c8bb6-5453-455d-932c-4e7bbbc553da",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08663d05-76b7-408a-9e06-ca8dd275c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.test_marginal_perm(test_data_dir, model, image_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deep-learning] *",
   "language": "python",
   "name": "conda-env-.conda-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
