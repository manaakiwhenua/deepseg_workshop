{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9d45fb-1003-43c8-a05f-266443730897",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "In this module we will extend the basic idea of convolutional neural networks from classifying an image to generating an alterbative image. In the context of remote sensing, this means ingesting one or more remote sensing images and generating a raster or map.\n",
    "\n",
    "This type of network is called an 'encoder-decoder'\n",
    "\n",
    "TODO - more here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e8e90-14eb-47a7-9c4b-fa81115eb617",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "First we'll import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83004978-edbb-4f28-8531-314495b9314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage import io, img_as_ubyte\n",
    "from skimage.transform import rescale\n",
    "\n",
    "import torch # Loads the CUDA libraries for tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "#from keras import backend as keras\n",
    "#from keras import utils as ku\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import glob\n",
    "\n",
    "import random\n",
    "\n",
    "# Keras utilities\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "#from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee4ef0-ab62-4dbb-93a6-0d623b5e6ba5",
   "metadata": {},
   "source": [
    "Now we'll import some utilities for visualisation etc. If you want to take a look at this code, uncomment the second line below before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc698e-f064-4515-b66a-d5ac2c16405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from files import utils\n",
    "# %load files/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9355f-6986-451c-8af7-29a1b39d57cc",
   "metadata": {},
   "source": [
    "## A simple model\n",
    "We'll start with the simplest encoder-decoder network:\n",
    "\n",
    "<img src=\"files/encoder_decoder.png\" width=\"500\"/>\n",
    "\n",
    "- the encoder consists of a single convolutional layer that looks for certain patterns and uses them to transform the input imagery into feature maps highlighting those salient patterns in the image\n",
    "- the decoder network also consists of just a single convolutional layer. This layer has one 'filter' per class, and this filter projects (collapses) the all the feature map outputs for each individual pixel into a single pixel value. In other words, it learns to assign a weight to each of the feature maps for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2671b-cd34-4fc9-8988-3fb09ba0d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_decoder(num_classes, input_size):\n",
    "    num_filters = 64 # 16 # also 4, 16\n",
    "    filter_size = 3 # also 4,5\n",
    "\n",
    "    kernel_init = initializers.RandomUniform(minval=-0.01, maxval=0.01) # For visualisation\n",
    "\n",
    "    # Preprocess the input\n",
    "    inputs = layers.Input(input_size, name='Input')\n",
    "    rescale = layers.Rescaling(1.0 / 255, name='Rescale')(inputs) # Rescale to 0..1\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = layers.Conv2D(num_filters, filter_size, padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init, name='Encoder')(rescale)\n",
    "\n",
    "    # Decoder\n",
    "    out = layers.Conv2D(num_classes, 1, activation=\"softmax\", padding=\"same\", kernel_initializer=kernel_init, name='Decoder')(conv1)\n",
    "\n",
    "    model = Model(inputs, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981ff6b-a5f6-404e-8023-bd08c1718a83",
   "metadata": {},
   "source": [
    "Let's build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1693a53-d5c4-405b-82f5-8a3ae5a43b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3 # (0=background; 1=nought; 2=cross)\n",
    "image_size = 32\n",
    "input_size = (image_size,image_size,3)\n",
    "# learning_rate = 1e-4 # Our go-to learning rate\n",
    "learning_rate = 0.01 # Fast learning\n",
    "\n",
    "model = encoder_decoder(num_classes, input_size)\n",
    "model.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n",
    "model.summary(line_length=80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a1bd7a-c35a-4a01-a83b-2c6a2b19334d",
   "metadata": {},
   "source": [
    "let's take a look at the initial model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c8a4a-53be-42be-9961-345b00891dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e44d01-7f57-4738-b050-5174495b508a",
   "metadata": {},
   "source": [
    "## Training\n",
    "Lets try out our hyper-simple model on some real data. We'll use noughts and crosses again, but this time we want the network to generate a mask that assigns the value 1 to noughts and 2 to crosses. Here's an example input and the expected output:\n",
    "\n",
    "<table><tr>\n",
    "        <td><img src=\"files/seg_data/seg_clean/train/images/65.png\" width=\"200\"/></td>\n",
    "        <td><img src=\"files/seg_data/seg_clean/train/visual/65.png_visual.png\" width=\"200\"/></td>\n",
    "</tr></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef9c77-5aa5-4920-b39d-a11f81b81761",
   "metadata": {},
   "source": [
    "Training is slightly more complex this time. First, we'll need a couple of data generators that prepare the input images and label masks for ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fb9aca-9dd9-4faf-a022-443c08a9f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,target_size):\n",
    "    # Yields batches of (image,mask) of batch_size\n",
    "\n",
    "    image_path = os.path.join(train_path, image_folder)\n",
    "    mask_path = os.path.join(train_path, mask_folder)\n",
    "    \n",
    "    i = 0\n",
    "    n = 0\n",
    "    (width, height) = target_size\n",
    "    \n",
    "    while True: # The caller will decide when to stop\n",
    "        # Generate a randomised list of (image, mask) filenames\n",
    "        files = list(zip(sorted(list(glob.glob(image_path + '/*'))), sorted(list(glob.glob(mask_path + '/*')))))\n",
    "        random.shuffle(files)\n",
    "        \n",
    "        for (filename, mask_filename) in files:\n",
    "            mask = io.imread(mask_filename)\n",
    "            if np.sum(mask) == 0: # ignore images with blank masks\n",
    "                pass\n",
    "            else:\n",
    "                img = io.imread(filename)\n",
    "                if i == 0:\n",
    "                    images = np.zeros((batch_size, img.shape[0], img.shape[1], img.shape[2]))\n",
    "                    masks = np.zeros((batch_size, mask.shape[0], mask.shape[1], 1))\n",
    "    \n",
    "                images[i] = img\n",
    "                masks[i] = np.expand_dims(mask[:,:,0],-1)\n",
    "                \n",
    "                i += 1\n",
    "                if i == batch_size:\n",
    "                    i = 0\n",
    "                    # images, masks = adjustData(images,masks,flag_multi_class,num_class) # No need\n",
    "                    if np.max(masks) > 0:\n",
    "                        yield (images, masks)\n",
    "\n",
    "def testGenerator(test_path, target_size, offset=0, num_files=0):\n",
    "    (width, height) = target_size\n",
    "\n",
    "    files = sorted(glob.glob(test_path + '/*'))\n",
    "\n",
    "    if num_files == 0:\n",
    "        num_files = len(files)\n",
    "    \n",
    "    i = 0\n",
    "    while i < num_files:\n",
    "        filename = files[i+offset]\n",
    "        img = io.imread(filename)\n",
    "        # img = np.reshape(img,(1,) + img.shape) # Make into a batch of size 1\n",
    "        img = np.expand_dims(img,0) # Make into a batch of size 1\n",
    "        i += 1\n",
    "        yield img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e72ad-c443-4a67-b3bb-b4f33944534a",
   "metadata": {},
   "source": [
    "Now we'll train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c2f3c-babd-4e8e-8dcd-752c5bbdb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "IMAGES_DIR = 'images/'\n",
    "LABELS_DIR = 'labels/'\n",
    "\n",
    "data_dir = \"files/seg_data/seg_clean/\"\n",
    "train_dir = data_dir + \"/train/\"\n",
    "# train_dir = data_dir + \"/train_debug/\" # One image only - should easily learn it to 100%\n",
    "test_dir = data_dir + \"/test/\" # All three bands contain the character\n",
    "out_dir = data_dir + \"out/\"\n",
    "\n",
    "#STEPS_PER_EPOCH = 50 # Entire training set\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "STEPS_PER_EPOCH = 50 # Entire training set\n",
    "# EPOCHS = 1 # Quick test\n",
    "# EPOCHS = 5 # Maximal for Unet64? No - it's a mess!\n",
    "EPOCHS = 20\n",
    "# EPOCHS = 100\n",
    "# EPOCHS = 10 # Learning anything???\n",
    "       \n",
    "train_gen = trainGenerator(TRAIN_BATCH_SIZE, train_dir, IMAGES_DIR, LABELS_DIR, target_size=(image_size,image_size))\n",
    "val_gen = trainGenerator(TRAIN_BATCH_SIZE, test_dir, IMAGES_DIR, LABELS_DIR, target_size=(image_size,image_size))\n",
    "\n",
    "# model.fit_generator(train_gen,steps_per_epoch=STEPS_PER_EPOCH,epochs=num_epochs, callbacks=[model_checkpoint], verbose=VERBOSITY)\n",
    "#model.fit_generator(train_gen, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, \n",
    "#                    validation_data = val_gen, validation_steps=STEPS_PER_EPOCH)\n",
    "\n",
    "model.fit(train_gen, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data = val_gen, validation_steps=STEPS_PER_EPOCH)\n",
    "\n",
    "print(\"\\n*** TRAINING COMPLETE ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce8314-c040-4b1a-9331-e9c69bcd58b4",
   "metadata": {},
   "source": [
    "## Testing\n",
    "For segmentation we are interested in the model's ability to generate the desired output. First, let's test the model on an image and see how the result compares to our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f073d6-5942-4084-85a9-59824cc99be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '65.png'\n",
    "test_image = test_dir + 'images/' + test_file\n",
    "test_mask = test_dir + 'labels/' + test_file\n",
    "\n",
    "img = io.imread(test_image)\n",
    "img = np.expand_dims(img,0) # make into a batch of size 1\n",
    "#results = list(model.predict_generator(testGene, 1, verbose=1))\n",
    "result = list(model.predict(img, 1, verbose=0))[0]\n",
    "# Convert the result to a mask\n",
    "mask = np.argmax(result, axis = 2).astype(np.uint8) # Finds the index of the highest class probability for each (x,y)\n",
    "\n",
    "# Plot the original image, the result, and the probabilities\n",
    "# TODO move to utils (\"plot_images\")\n",
    "figure, axis = plt.subplots(1,4)\n",
    "[axis[i].set_axis_off() for i in range (0,4)]\n",
    "[axis[i].figure.set_figwidth(10) for i in range (0,4)]\n",
    "axis[0].imshow(img[0])\n",
    "axis[1].imshow(mask)\n",
    "axis[2].imshow(result[:,:,1], cmap='gray') # class = 1 (nought)\n",
    "axis[3].imshow(result[:,:,2], cmap='gray') # class = 2 (cross)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4fbf8-93d8-4cdc-9682-7c8d4846b5e5",
   "metadata": {},
   "source": [
    "## Visualising performance\n",
    "In this simple example it's not hard to see where the model was correct/wrong, \n",
    "but this obviously gets a lot more difficult as the complexity of the imagery and/or labelling rises.\n",
    "\n",
    "One way to solve this is to generate a comparison of the desired versus generated results. In the visualisation below:\n",
    "- Blue = true negative (both truth and prediction are class 0/background)\n",
    "- White = true positive (both truth and prediction are the same non-zero class)\n",
    "- Yellow = wrong class (truth and prediction are *different* non-zero class)\n",
    "- Red = false negative (truth is non-zero, predicted is zero)\n",
    "- Green = false positive (truth is zero, predicted is non-zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc170e7d-7b3b-4a83-a1ae-7ce631cc133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_truth = io.imread(test_mask)\n",
    "error_shape = im_truth.shape\n",
    "if len(error_shape) == 2:\n",
    "    error_shape = error_shape + (3,)\n",
    "im_error = np.zeros(error_shape)\n",
    "\n",
    "# Grab the first channel only if RGB masks\n",
    "if len(im_truth.shape) == 3: im_truth = im_truth[:,:,0]\n",
    "if len(mask.shape) == 3: im_predicted = im_predicted[:,:,0]\n",
    "\n",
    "im_error[:,:,0] = (im_truth > 0) # Red: truth is non-zero\n",
    "im_error[:,:,1] = (mask > 0) # Green: prediction is non-zero\n",
    "im_error[:,:,2] = (mask == im_truth) # Blue: prediction = truth\n",
    "\n",
    "plt.imshow(im_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56689f-1451-4b75-a185-3996bfec2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_outputs(model, test_image, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60efdb-ca2e-418d-a543-b1a25ac71688",
   "metadata": {},
   "source": [
    "## Measuring performance\n",
    "There are several ways to objectively measure (pixel) performance, including:\n",
    "- Overall accuracy: number of correct pixels\n",
    "- IOU (intersection over union): percentage of (truth U prediction) pixels that overlap, for each (non-zero) class\n",
    "- Confusion: shows the patterns of correct and incorrect results by class\n",
    "- Precision: percentage of truth pixels of a given class that were correctly classified (versus false negatives)\n",
    "- Recall: percentage of predictions for a given class that were correctly classified (versus false positives)\n",
    "\n",
    "In a mapping context, we may also be interested in measuring accuracy per-polygon basis; IOU is particularly useful in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07281c68-3059-4e77-b2d1-3d047d2d3bd6",
   "metadata": {},
   "source": [
    "## A more complex network\n",
    "The network we just trained is extremely simple - it is just one layer \"deep\". We will increase its abstraction ability by adding more layers.\n",
    "Take a look at the network definition below.\n",
    "\n",
    "<img src=\"files/unet_lite.png\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d78216-c117-4d20-8cc3-27f6e17db07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_lite(num_classes, input_size):\n",
    "    num_filters = 8 #64 # 16 # also 4, 16\n",
    "    filter_size = 3 # also 4,5\n",
    "    # num_hidden = 64 # also 16?)\n",
    "\n",
    "    kernel_init = initializers.RandomUniform(minval=-0.01, maxval=0.01) # For visualisation\n",
    "\n",
    "    # Preprocess the input\n",
    "    inputs = layers.Input(input_size, name='Inputs')\n",
    "    rescale = layers.Rescaling(1.0 / 255, name='Rescale')(inputs) # Rescale to 0..1\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = layers.Conv2D(num_filters, filter_size, padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init, name='Encoder-1')(rescale)\n",
    "    down1 = layers.MaxPooling2D(name='MaxPool')(conv1)\n",
    "\n",
    "    head = layers.Conv2D(num_filters * 2, filter_size, padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init, name='Encoder-2')(down1)\n",
    "\n",
    "    # Decoder\n",
    "    upsample1 = layers.UpSampling2D(size=(2, 2),name='Upsample')(head)\n",
    "    up1 = layers.Conv2D(num_filters * 2, filter_size, padding=\"same\", activation=\"relu\", kernel_initializer=kernel_init, name = 'Decoder-1')(upsample1)\n",
    "\n",
    "    merge1 = layers.concatenate([conv1,up1], axis=3, name='Merge') # Merge the encoder and decoder outputs with the same scale\n",
    "\n",
    "    out = layers.Conv2D(num_classes, 1, activation=\"softmax\", padding=\"same\", kernel_initializer=kernel_init, name='Decoder-2')(merge1)\n",
    "   \n",
    "    model = Model(inputs, out, name=\"Unet-lite\")\n",
    "    return model\n",
    "\n",
    "model = unet_lite(num_classes, input_size)\n",
    "model.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83001511-11f6-4e82-8d1c-7a876518363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_gen, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data = val_gen, validation_steps=STEPS_PER_EPOCH)\n",
    "\n",
    "print(\"\\n*** TRAINING COMPLETE ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1869551-658d-49c8-ae06-e3ab1e641c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_outputs(model, test_image, image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd092ea1-b16e-49ad-8562-266e860b8dba",
   "metadata": {},
   "source": [
    "## Unet\n",
    "The original Unet is exactly like our light version except it is several layers deep. Let's see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099c7d4c-6979-4415-8641-44924485c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(num_classes, input_size):\n",
    "    width = 16 # originally 64\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    rescale = layers.Rescaling(1.0 / 255)(inputs) # Rescale to 0..1 - BIM\n",
    "    conv1 = Conv2D(width, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(rescale) # (inputs)\n",
    "    conv1 = Conv2D(width, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)   \n",
    "    conv2 = Conv2D(width*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(width*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)    \n",
    "    conv3 = Conv2D(width*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(width*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)    \n",
    "    conv4 = Conv2D(width*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(width*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "    conv5 = Conv2D(width*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(width*16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(width*8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(width*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(width*8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(width*4, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(width*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(width*4, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(width*2, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(width*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(width*2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(width, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    \n",
    "    conv9 = Conv2D(width, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(width, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2*num_classes, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(num_classes, 1, activation = 'softmax', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "        \n",
    "    model = Model(inputs, conv10, name='Unet64')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = unet(num_classes, input_size)\n",
    "model.compile(optimizer = Adam(learning_rate = learning_rate), loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3ec35-85f2-4e99-be32-0f273f3a2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_gen, steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data = val_gen, validation_steps=STEPS_PER_EPOCH)\n",
    "\n",
    "print(\"\\n*** TRAINING COMPLETE ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d853822-60f9-4e30-93bd-18030c1e7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.report_outputs(model, test_image, image_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-deep-learning] *",
   "language": "python",
   "name": "conda-env-.conda-deep-learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
